{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshimaBagaria/Lin-Reg_YESBank/blob/main/AB_YesBank_Linear_Reg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - YES Bank Closing Price Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 - Ashima Bagaria**\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes Bank is a banking company that was founded in 2004 that offers a wide\n",
        "range of differentiated products for its corporate and retail customers through\n",
        "retail banking and asset management services.\n",
        "\n",
        "It is also a publically traded company. That provides an opportunity for anyone to invest in Yes bank and become a shareholder. But at the same time, it means that the valuation of the company is now in the hands of investors and speculators as share prices are often heavily impacted by public opinion.\n",
        "\n",
        "I have used yes bank stock price data set. This dataset contains 5\n",
        "different features that can be used for predicting close price prediction\n",
        "using machine learning. I have aimed to build the most efficient machine learning regression model for closing price prediction."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/AshimaBagaria/Lin-Reg_YESBank"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A well-known bank in the Indian financial industry,Yes Bank, has been in the headlines since 2018. Due to the fraud case involving its founder, Rana Kapoor, it was intriguing to observe how it has affected the company's stock prices and whether Time series models or other prediction models could adequately account for such circumstances. Since the bank's foundation, this dataset has included closing, starting, highest, and lowest stock values for each month.My key goal is to predict the stock's monthly closing price given the other independent values."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import (r2_score,mean_squared_error,mean_absolute_percentage_error, mean_absolute_error)\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "\n",
        "# mount drive to load dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df_Master = pd.read_csv(\"/content/drive/MyDrive/data_YesBank_StockPrices.csv\")\n",
        "df_Master.head()                                                                #Shows first 5 rows\n",
        "\n",
        "# We can see from the dataframe, all the columns we have contain numerical data. There is no categorical data present.                                                                           # displays first five instances of the dataframe."
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "\n",
        "print('\\n', f'The number of rows dataset is : {df_Master.shape[0] }')\n",
        "print('\\n', f'The number of columns of the dataset is : {df_Master.shape[1] }')\n",
        "print('\\n', f'The dimension of the dataset is : {df_Master.shape }')\n",
        "                                                                           #To get number of rows and columns in the dataset"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df_Master.info()\n"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Values"
      ],
      "metadata": {
        "id": "Yk1QQZGYv0QG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_Master.isna().sum()                                                          #No null values found"
      ],
      "metadata": {
        "id": "2x3iFFaXv44G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "\n",
        "dups = df_Master.pivot_table(index = ['Open','High','Low','Close'], aggfunc ='size')\n",
        "print(dups)                                                                     #No duplicates found"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After loading the data, I observed the\n",
        "dataset by checking a few of the first and last rows. There are 185 rows and 5\n",
        "features columns in our dataset.\n",
        "\n",
        "Lets understand the features present in our dataset.\n",
        "\n",
        "• Date: It denotes date of investment done (in our case we have\n",
        "month and year).\n",
        "\n",
        "• Open: Open means the price at which a stock started trading\n",
        "when the opening bell rang.\n",
        "\n",
        "• High: High refer to the maximum prices in a given time\n",
        "period.\n",
        "\n",
        "• Low: Low refer to the minimum prices in a given time\n",
        "period.\n",
        "\n",
        "• Close: Close refers to the price of an individual stock at the\n",
        "end of the considered time period.\n",
        "Exploratory Data Analysis:-\n",
        "\n",
        "Also,\n",
        "\n",
        "The Given Date in data is of Month-year format (mmm-yy) and has to be  converted to\n",
        "proper date of YYYY-MM-DD and given date column has dtype as object\n",
        "converting it into date time format.\n",
        "\n",
        "Null values Treatment:\n",
        "Our dataset does not contain null values which tend to affect\n",
        "our accuracy. If we had null values, we could drop them or\n",
        "imput them with mean or median depending on the situation.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "#Displaying all columns in the dataset\n",
        "df_Master.columns\n"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "\n",
        "df_Master.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_Master['Date']\n",
        "\n",
        "# converting the format of date\n",
        "from datetime import datetime\n",
        "df_Master['Date'] = pd.to_datetime(df_Master['Date'].apply(lambda x: datetime.strptime(x, '%b-%y')))     # this converts date to a yyyy-mm-dd\n",
        "\n",
        "df_Master.set_index('Date', inplace=True)                                                                # setting Date column as index.\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "70Me1yNcYl1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart (1 - 4): Checking for outliers using Box Plot"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "x = 0                                                                            #to keep track of chart number\n",
        "colorscheme=['aqua','powderblue','teal','lightslategrey','white']\n",
        "for column in df_Master.columns:\n",
        "  plt.figure(figsize=(3,3))\n",
        "  sns.boxplot(df_Master[column], color = colorscheme[x])\n",
        "  plt.xlabel(column)\n",
        "  x=x+1\n",
        "  plt.title(\"Box Plot : \" + str(x) , fontsize =8)\n",
        "  plt.show()\n",
        "  print()\n"
      ],
      "metadata": {
        "id": "vfMsZb4u0Wat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked box plots to check all features for presence of outliers.Boxplots are a standardized way of displaying the distribution of data based on a five number summary (“minimum”, first quartile (Q1), median, third quartile (Q3), and “maximum”). This type of plot is used to easily detect outliers."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see there are some outliers present in our data. We will need to deal with these before proceeding to modelling."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above insight will help us deal with outliers appropriately. Presence of outliers affects the accuracy of a linear regression."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chart (5-9) :Univariate Analysis**"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = 5                                                                           #Keeps track of chart number\n",
        "colorscheme=['maroon','orange','green','brown','white']\n",
        "\n",
        "for col in df_Master.columns:\n",
        "  if str(col) != \"Date\":\n",
        "    plt.figure(figsize=(4,3))\n",
        "    sns.distplot(df_Master[str(col)], color = colorscheme[x-5])                 #Using index of the colorscheme\n",
        "    x=x+1\n",
        "    plt.title(\"Chart: \" + str(x) +\" -  \"+ str(col) + ' Value Data Distribution')\n",
        "    plt.xlabel(str(col) +' Price')\n",
        "    plt.axvline(df_Master[str(col)].mean(), color='maroon', linestyle='dashed', linewidth=1)  #Plotting mean value\n",
        "    plt.axvline(df_Master[str(col)].median(), color='black', linestyle='dashed', linewidth=1) #Plotting median value\n",
        "    plt.show()\n",
        "    print()\n",
        "\n",
        "x=9\n",
        "plt.plot(df_Master['Close'])\n",
        "plt.title(\"Chart: \" + str(x) +\" - Closing value over time\")\n",
        "plt.xlabel('Closing Price')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is often used to illustrate the major features of the distribution of the data in a convenient form. It is also useful when dealing with large data sets (greater than 100 observations). It can help detect any unusual observations (outliers) or any gaps in the data."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can clearly see that *these distributions are positively skewed*. The mean and median are at significant distance from each other."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the gained insight we understand the we need to transform them into something close to a Normal Distribution as our models give optimal results that way."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart (9-11) Bivariate Analysis"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colorscheme=['blue','purple','teal','lightslategrey','white']\n",
        "x=9                                                                             #Keeps track of chart number\n",
        "for col in df_Master.columns[:-1]:\n",
        "  if(str(col)!= \"Date\"):\n",
        "    fig = plt.figure(figsize=(7,5))\n",
        "    ax = fig.gca()\n",
        "    plt.scatter(df_Master[str(col)], df_Master['Close'],color = colorscheme[x-9]) #Plotting each feature vs closing value to check for linearity\n",
        "    x=x+1\n",
        "    plt.title(\"Chart: \" + str(x))\n",
        "    plt.xlabel(str(col))\n",
        "    plt.ylabel('Close')\n",
        "    ax.set_title(f'{str(col)} vs Close')\n",
        "\n",
        "\n",
        "    z = np.polyfit(df_Master[col], df_Master['Close'], 1)                       #Numpy polyfit() is a function that is used to fit the data within a polynomial function.\n",
        "    y_hat = np.poly1d(z)(df_Master[col])\n",
        "\n",
        "    plt.plot(df_Master[col], y_hat, \"red\", lw=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A scatter plot uses dots to represent values for two different numeric variables. Scatter plots are used to observe relationships between variables.Hence scatter plot was the best choice to establish a relationship between the independent and dependent variables."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All independent variables share a linear relationship with the dependent variable 'Close'. We also understand the presence of outliers from the graph"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can infer that 'Close' value is directly dependent on the opening,highest and lowest values."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 12: Multivariate Analysis"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Correlation Heatmap')\n",
        "cor = sns.heatmap(df_Master.corr(),annot=True)                       #Heat map to check for correlation between all features of the dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation heatmap is a graphical tool that displays the correlation between multiple variables as a color-coded matrix."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above heatmap we understand that the independent variables are highly correlated."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight gained here will help us deal with multicollinearity. Multicollinearity does not effect the model performance but effects the model predictability."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 13 - 16 (Log Transformation)"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using log transformation on these features using np.log().\n",
        "colorscheme=['blue','purple','teal','lightslategrey','white']\n",
        "x=13\n",
        "for col in df_Master.columns:\n",
        "  plt.figure(figsize=(5,3))\n",
        "  sns.distplot(np.log10(df_Master[col]), color= colorscheme[x-13])              #Applying log10 on each column\n",
        "  x=x+1\n",
        "  plt.title(\"Chart \" + str(x)  )\n",
        "  plt.xlabel(col, fontsize=13)\n",
        "  plt.ylabel('count')\n",
        "\n",
        "  print()\n",
        "\n",
        "  # Plotting the mean and the median.\n",
        "  plt.axvline(np.log10(df_Master[col]).mean(),color='red',linewidth=2)          #Plotting mean\n",
        "  plt.axvline(np.log10(df_Master[col]).median(),color='yellow',linewidth=1.5)   #Plotting median\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram is useful when the data is numerical. We wanted to see the shape of the data's distribution and determine whether the output of a process is distributed approximately normally."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From univariate distribution we realised that our data was positively skewed.\n",
        "So we do a log transform on it and plot it as seen in the right chart. This makes it approximate normal distribution and is optimal for our model’s performance. Now our mean and median are nearly equal."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 16-19 (Checking for outliers with log transformed data)"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = 16\n",
        "for col in df_Master.columns:\n",
        "  plt.figure(figsize=(3,3))\n",
        "  sns.boxplot(np.log10(df_Master[col]), color ='salmon')                                      #Converting column values to their log values\n",
        "  plt.title(\"Chart :\"+str(x))\n",
        "  x=x+1\n",
        "  plt.xlabel(col, fontsize=8)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked box plots to check all features for presence of outliers.Boxplots are a standardized way of displaying the distribution of data based on a five number summary (“minimum”, first quartile (Q1), median, third quartile (Q3), and “maximum”). This type of plot is used to easily detect outliers."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above plots show that on log transformation , the outliers have been completely eliminated."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removal of outliers completely in a small data set may affect our prediction negatively. Hence, we shall not apply the log transformation."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dealing with multicollinearity"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using VIF(Variation Inflation Factor) to see the correlation between independent variables\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = df_Master.columns[:-1]                                      #Excluding the dependent column\n",
        "\n",
        "# calculating VIF for each feature\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(df_Master.values, i)\n",
        "                          for i in range(len(df_Master.columns[:-1]))]\n",
        "\n",
        "print(vif_data)"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feature Engineering"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Since the features are highly correlated and the VIF is quite high for all of them, We can treat them as a single feature and use their mean to reach to our desired conclusion.\n",
        "\n",
        "# Creating a new feature which would be the mean of all the independent features\n",
        "df_Master['featuredColumn'] = df_Master[['Open', 'High', 'Low']].mean(axis=1).round(4)\n",
        "df_Master.head()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 20: FeaturedColumn vs Closing Value"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(10,5))\n",
        "ax = fig.gca()\n",
        "plt.scatter(df_Master['featuredColumn'], df_Master['Close'])\n",
        "plt.xlabel('featuredColumn')\n",
        "plt.ylabel('Closing Value')\n",
        "plt.title('featuredColumn vs Close')\n",
        "z = np.polyfit(df_Master['featuredColumn'], df_Master['Close'], 1)\n",
        "y_hat = np.poly1d(z)(df_Master['featuredColumn'])                                 #Plotting fit line\n",
        "plt.plot(df_Master['featuredColumn'], y_hat, \"green\", lw=1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dependent variable 'Close' follows a linear relationship with the new feature engineered column which is an aggregate of all the independent columns"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train Test Split"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using 80% for training and 20% for testing\n",
        "\n",
        "y = y = df_Master['Close'].values\n",
        "x = df_Master.dropna().drop(['Close','Open','High','Low'], axis=1)              #Considering only the aggregate of the independent columns\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scaling the data"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "\n",
        "#Since there is only one feature we are dealing with, this may be skipped"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - Linear Regression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_lr = LinearRegression()\n",
        "model_lr.fit(x_train, y_train)                                                   #Training our model\n",
        "\n",
        "LinearRegression()\n",
        "\n",
        "# make predictions\n",
        "lr_y_pred = model_lr.predict(x_test)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "print(\"***********************************************************************************************\")\n",
        "\n",
        "regr_mae = round(mean_absolute_error(y_test, lr_y_pred),5)\n",
        "print(\"The Mean Absolute Error of our Model is {}\".format(regr_mae))\n",
        "\n",
        "\n",
        "regr_mse  = round(mean_squared_error(y_test,lr_y_pred),5)\n",
        "print(\"Mean squared Error :\" , regr_mse)\n",
        "\n",
        "regr_rmse = round(np.sqrt(regr_mse),5)\n",
        "print(\"The Root Mean Absolute Error of our Model is {}\".format(regr_rmse))\n",
        "\n",
        "regr_r2 = round(r2_score(y_test, lr_y_pred),5)\n",
        "print(\"The R2 of our model is {}\".format(regr_r2))\n",
        "\n",
        "\n",
        "regr_adj_r2 = round(1-(1-r2_score(y_test,lr_y_pred))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)),5)\n",
        "print(f\"Adjusted R2 score : {regr_adj_r2}\")\n",
        "\n",
        "regr_mape = round(mean_absolute_percentage_error(lr_y_pred, y_test),5)\n",
        "print('mean absolute percentage error: {}\\n\\n\\n'.format(regr_mape))\n",
        "\n",
        "print(\"***********************************************************************************************\")\n",
        "\n",
        "\n",
        "#Plotting predicted values vs the actual test data\n",
        "\n",
        "plt.figure(figsize=(12,7))\n",
        "plt.plot(lr_y_pred, color = \"blue\")\n",
        "plt.plot(np.array(y_test) ,color = \"red\")\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.xlabel('Test Data')\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Actual vs Predicted Closing price Linear regression\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2: Lasso Regression with Grid Search CV"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the model with some base values.\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "lasso  = Lasso(alpha=0.001 , max_iter= 3000)\n",
        "\n",
        "# Fitting the model on our training data.\n",
        "lasso.fit(x_train, y_train)\n",
        "\n",
        "# Printing the intercept and coefficients.\n",
        "lasso.intercept_\n",
        "lasso.coef_\n",
        "\n",
        "# Cross validation. optimizing our model by finding the best value of our hyperparameter.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "lasso_param_grid = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,0.005,0.006,0.007,0.01,0.015,0.02,1e-1,1,5,10,20,30,40,45,50]}  # list of parameters.\n",
        "\n",
        "lasso_regressor = GridSearchCV(lasso, lasso_param_grid, scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "lasso_regressor.fit(x_train, y_train)\n",
        "\n",
        "GridSearchCV(cv=5, estimator=Lasso(alpha=0.0001, max_iter=5000),\n",
        "             param_grid={'alpha': [1e-15, 1e-13, 1e-10, 1e-08, 1e-05, 0.0001,\n",
        "                                   0.001, 0.005, 0.006, 0.007, 0.01, 0.015,\n",
        "                                   0.02, 0.1, 1, 5, 10, 20, 30, 40, 45, 50]},\n",
        "             scoring='neg_mean_squared_error')\n",
        "\n",
        "# getting the best parameter\n",
        "lasso_regressor.best_params_          # after several iterations and trials, we get this value as best parameter value.\n",
        "\n",
        "{'alpha': 1e-05}\n",
        "\n",
        "# getting the best score\n",
        "print(lasso_regressor.best_score_)\n",
        "\n",
        "# Predicting on the test dataset.\n",
        "y_pred_lasso = lasso_regressor.predict(x_test)\n",
        "print(y_pred_lasso)"
      ],
      "metadata": {
        "id": "UdoYC0rXabaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"***********************************************************************************************\")\n",
        "\n",
        "lasso_mae_score = round(mean_absolute_error(y_test, y_pred_lasso),5)\n",
        "print(\"The Mean Absolute Error of our Model is {}\".format(lasso_mae_score))\n",
        "\n",
        "lasso_mse_score  = round(mean_squared_error(y_test,y_pred_lasso),5)\n",
        "print(\"Mean squared Error :\" , lasso_mse_score)\n",
        "\n",
        "lasso_rmse_score = round(np.sqrt(lasso_mse_score),5)\n",
        "print(\"The Root Mean Absolute Error of our Model is {}\".format(lasso_rmse_score))\n",
        "\n",
        "lasso_r2score = round(r2_score(y_test, y_pred_lasso),5)\n",
        "print(\"The R2 of our model is {}\".format(lasso_r2score))\n",
        "\n",
        "\n",
        "lasso_adj_r2 = round(1-(1-r2_score(y_test,y_pred_lasso))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)),5)\n",
        "\n",
        "lasso_mape = round(mean_absolute_percentage_error(y_pred_lasso, y_test),5)\n",
        "print('mean absolute percentage error: {}\\n\\n\\n'.format(lasso_mape))\n",
        "print(\"***********************************************************************************************\")\n",
        "\n",
        "\n",
        "\n",
        "#Plotting predicted values vs the actual test data\n",
        "\n",
        "plt.figure(figsize=(12,7))\n",
        "plt.plot(y_pred_lasso, color = \"pink\")\n",
        "plt.plot(np.array(y_test) ,color = \"red\")\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.xlabel('Test Data')\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Actual vs Predicted Closing price Lasso regression\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - Ridge Regression with Grid search CV"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "ridge = Ridge()         # iitializing the model\n",
        "\n",
        "# initiating the parameter grid for alpha (regularization strength).\n",
        "ridge_param_grid = {'alpha': [1e-15,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,0.3,0.7,1,1.2,1.33,1.365,1.37,1.375,1.4,1.5,1.6,1.8,2.5,5,10,20,30,40,45,50,55,60,100]}\n",
        "\n",
        "# cross validation.\n",
        "ridge_regressor = GridSearchCV(ridge, ridge_param_grid, scoring='neg_mean_squared_error', cv=5)\n",
        "ridge_regressor.fit(x_train,y_train)\n",
        "\n",
        "GridSearchCV(cv=3, estimator=Ridge(),\n",
        "             param_grid={'alpha': [1e-15, 1e-10, 1e-08, 1e-05, 0.0001, 0.001,\n",
        "                                   0.01, 0.3, 0.7, 1, 1.2, 1.33, 1.365, 1.37,\n",
        "                                   1.375, 1.4, 1.5, 1.6, 1.8, 2.5, 5, 10, 20,\n",
        "                                   30, 40, 45, 50, 55, 60, 100]},\n",
        "             scoring='neg_mean_squared_error')\n",
        "\n",
        "# finding the best parameter value (for alpha)\n",
        "ridge_regressor.best_params_\n",
        "\n",
        "{'alpha': 0.01}\n",
        "\n",
        "# getting the best score for optimal value of alpha.\n",
        "ridge_regressor.best_score_\n",
        "\n",
        "# predicting on the test dataset now.\n",
        "y_pred_ridge = ridge_regressor.predict(x_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "\n",
        "# evaluating performance.\n",
        "print(\"***********************************************************************************************\")\n",
        "\n",
        "MAE_ridge = round(mean_absolute_error(y_test,y_pred_ridge),5)\n",
        "print(f\"Mean Absolute Error : {MAE_ridge}\")\n",
        "\n",
        "MSE_ridge  = round(mean_squared_error(y_test,y_pred_ridge),5)\n",
        "print(\"Mean squared Error :\" , MSE_ridge)\n",
        "\n",
        "RMSE_ridge = round(np.sqrt(MSE_ridge),5)\n",
        "print(\"Root Mean squared Error :\" ,RMSE_ridge)\n",
        "\n",
        "R2_ridge = round(r2_score(y_test,y_pred_ridge),5)\n",
        "print(\"R2 score :\" ,R2_ridge)\n",
        "\n",
        "Adjusted_R2_ridge = round(1-(1-r2_score(y_test, y_pred_ridge))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)),5)\n",
        "\n",
        "MAPE_ridge = round(mean_absolute_percentage_error(y_pred_ridge, y_test),5)\n",
        "print('mean absolute percentage error: {}\\n\\n\\n'.format(MAPE_ridge))\n",
        "print(\"***********************************************************************************************\")\n",
        "\n",
        "\n",
        "#Plotting predicted values vs the actual test data\n",
        "\n",
        "plt.figure(figsize=(12,7))\n",
        "plt.plot(y_pred_ridge, color = \"purple\")\n",
        "plt.plot(np.array(y_test) ,color = \"red\")\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.xlabel('Test Data')\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Actual vs Predicted Closing price Ridge Regression\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 4: Elastic net Regression"
      ],
      "metadata": {
        "id": "U9wRCP4Af_ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing and initializing Elastic-Net Regression.\n",
        "from sklearn.linear_model import ElasticNet\n",
        "elasticnet_model = ElasticNet(alpha=0.001, l1_ratio=0.5)\n",
        "\n",
        "# initializing parameter grid.\n",
        "elastic_net_param_grid = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,0.001,0.01,0.02,0.03,0.04,1,5,10,20,40,50,60,100],\n",
        "                          'l1_ratio':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]}\n",
        "\n",
        "# cross-validation.\n",
        "elasticnet_regressor = GridSearchCV(elasticnet_model, elastic_net_param_grid, scoring='neg_mean_squared_error',cv=5)\n",
        "elasticnet_regressor.fit(x_train, y_train)\n",
        "\n",
        "# finding the best parameter\n",
        "elasticnet_regressor.best_params_\n",
        "pred_y_elNet = elasticnet_regressor.predict(x_test)\n"
      ],
      "metadata": {
        "id": "dmRDxXfpgKAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "6nUafxV1grrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate predictions\n",
        "print(\"***********************************************************************************************\")\n",
        "\n",
        "MAE_en = round(mean_absolute_error(y_test,pred_y_elNet),5)\n",
        "print(f\"Mean Absolute Error : {MAE_en}\")\n",
        "\n",
        "MSE_en= round(mean_squared_error(y_test,pred_y_elNet),5)\n",
        "print(\"Mean squared Error :\" , MSE_en)\n",
        "\n",
        "RMSE_en = round(np.sqrt(MSE_en),5)\n",
        "print(\"Root Mean squared Error :\" ,RMSE_en)\n",
        "\n",
        "R2_en = round(r2_score(y_test,pred_y_elNet),5)\n",
        "print(\"R2 score :\" ,R2_en)\n",
        "\n",
        "Adjusted_R2_en = round(1-(1-r2_score(y_test, pred_y_elNet))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)),5)\n",
        "print(\"Adjusted R2 score: \", Adjusted_R2_en)\n",
        "\n",
        "MAPE_en = round(mean_absolute_percentage_error(pred_y_elNet, y_test),5)\n",
        "print('mean absolute percentage error: {}\\n\\n\\n'.format(MAPE_en))\n",
        "\n",
        "print(\"***********************************************************************************************\")\n"
      ],
      "metadata": {
        "id": "nYWBW7qegwc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataframe with evaluation metrics\n",
        "model_report = pd.DataFrame(data={'model':['Linear Regression','Ridge Regression','Lasso Regression','elastic net regression'], 'mae':[regr_mae,MAE_ridge,lasso_mae_score,MAE_en], 'mse':[regr_mse,MSE_ridge,lasso_mse_score,MSE_en],'rmse':[regr_rmse,RMSE_ridge,lasso_rmse_score,RMSE_en],'r2_score':[regr_r2,R2_ridge,lasso_r2score,R2_en],'Adj_r2':[regr_adj_r2,Adjusted_R2_ridge,lasso_adj_r2,Adjusted_R2_en],'MAPE':[regr_mape,MAPE_ridge,lasso_mape,MAPE_en]})\n",
        "model_report\n",
        ""
      ],
      "metadata": {
        "id": "UImvMI9PfVEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparing Values**"
      ],
      "metadata": {
        "id": "tF8J7jShhu7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "actual_pred_df = pd.DataFrame(y_test,pred_y_elNet).reset_index().rename(columns = {'index':'Actual values',0:'Elastic Net Predicted values'})\n",
        "actual_pred_df.head(10)"
      ],
      "metadata": {
        "id": "Q51IFmU56o72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting results of all models**"
      ],
      "metadata": {
        "id": "E4myHujeE-FG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the predicted values of all the models against the true values.\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.plot(y_test, linewidth=1,color='green')\n",
        "plt.plot(lr_y_pred)\n",
        "plt.plot(y_pred_lasso)\n",
        "plt.plot(y_pred_ridge)\n",
        "plt.plot(pred_y_elNet)\n",
        "plt.legend(['linear','lasso','ridge','elastic_net'])\n",
        "plt.title('Actual vs Predicted Closing Price values by various Algorithms', weight = 'bold',fontsize=16)\n",
        "plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "mqg5rVvUFFC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Homoscedasticity is an assumption in linear regression algorithm.\n",
        "# Homoscedasticity means that the model should perform well on all the datapoints.\n",
        "\n",
        "# Plotting the residuals(errors) against actual test data.\n",
        "residuals = np.log10(y_test) - np.log10(pred_y_elNet)\n",
        "plt.scatter(y_test,residuals,c='red')\n",
        "plt.title('Actual Test data vs Residuals (Elastic Net)')\n",
        "\n",
        "\n",
        "z = np.polyfit(y_test, residuals, 1)                       #Numpy polyfit() is a function that is used to fit the data within a polynomial function.\n",
        "y_hat = np.poly1d(z)(y_test)\n",
        "\n",
        "plt.plot(y_test, y_hat, \"blue\", lw=1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nmid5u5EdMoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " (1) Implemented regression model to predict Yes Bank stock’s closing price of the month. Data Cleaning steps were perfomed and missing values, duplicate records and outliers were dealt with.\n",
        " (2) While performing Feature Engineering, we checked for the optimal conditions/assumptions for regression – Linear dependence was checked, MultiCollinearity was reduced and variables were converted to Normal distribution via log transform and the assumption of homosdasceticity was verified for linear regression. One hot encoding was performed for categorical variables and scaling was done using Standardscaler.\n",
        " (3) Several Regression models such as linear regression, elastic net regression etc. were implemented. These models, then were optimized using GridSearchCV. Metrics like RMSE, MSE, MAE, R2 and adjusted R2 were found for evaluation of the above models. Elastic net regression was found to be the best performing model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " Using data visualization on our target variable, we can clearly see the impact\n",
        "of 2018 fraud case involving Rana Kapoor as the stock prices decline\n",
        "dramatically during that period.\n",
        " There is a high correlation between the dependent and independent variables.\n",
        "This is a signal that our dependent variable is highly dependent on our\n",
        "features and can be predicted accurately from them.\n",
        " We implemented several models on our dataset in order to be able to predict\n",
        "the closing price and found that Elastic Net regressor is the best\n",
        "performing model with Adjusted R2 score value of 0.9932 and it scores\n",
        "well on all evaluation metrics.\n",
        " All of the implemented models performed quite well on our data giving\n",
        "us the accuracy of over 99%.\n",
        " We checked for presence of Heterodasceticity in our dataset by plotting the\n",
        "residuals against the Elastic Net model predicted value and found that there is\n",
        "no Heterodasceticity present. Our model is performing well on all datapoints.\n",
        " With our model making predictions with such high accuracy even on unseen\n",
        "test data , we can confidently deploy this model for further predictive tasks\n",
        "using future real data.\n",
        " There are some outliers in our features however this being a very small\n",
        "dataset, dropping those instances will lead to loss of information.\n",
        " We found that there is a rather high correlation between our independent\n",
        "variables. This multicollinearity however is unavoidable here as the dataset\n",
        "is very small.\n",
        " We found that the distribution of all our variables is positively skewed. so we\n",
        "performed log transformation on them."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}